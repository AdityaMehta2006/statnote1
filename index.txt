<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Guide to Statistical Estimators</title>
    
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for LaTeX Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMQNogPSesLpvJxifG/T8LC5e82CE0CYfownAIMPxlVdFsVdk" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <!-- Add script to trigger rendering after DOM is loaded -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                // Delimiter options for KaTeX auto-rendering
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ],
                // To prevent errors from stopping rendering
                throwOnError : false
            });
        });
    </script>

    <style>
        /* General Styling - Dark Mode */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #121212; /* Dark background */
            color: #e0e0e0; /* Light grey text */
            line-height: 1.7;
            margin: 0;
            padding: 20px;
        }

        /* Main Container */
        .container {
            max-width: 900px;
            margin: 20px auto;
            padding: 20px;
        }

        /* Header Styling */
        header {
            text-align: center;
            border-bottom: 2px solid #333; /* Darker border */
            padding-bottom: 20px;
            margin-bottom: 40px;
        }
        header h1 {
            font-size: 2.8rem;
            color: #ffffff; /* White header */
            font-weight: 700;
        }
        header p {
            font-size: 1.2rem;
            color: #a0a0a0; /* Lighter grey for subtitle */
        }

        /* Section Card Styling */
        .card {
            background-color: #1e1e1e; /* Slightly lighter dark background for cards */
            border-radius: 12px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            border: 1px solid #333;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
        }
        .card h2 {
            font-size: 1.8rem;
            color: #ffffff;
            border-bottom: 2px solid #4db6ac; /* Teal accent color */
            padding-bottom: 10px;
            margin-top: 0;
        }
        .card h3 {
            font-size: 1.3rem;
            color: #f5f5f5;
            margin-top: 25px;
        }

        /* Definition and Example Boxes */
        .definition, .example-box, .proof-box, .exam-question {
            padding: 20px;
            margin-top: 20px;
            border-radius: 8px;
            border-left: 5px solid;
            background-color: rgba(255, 255, 255, 0.05); /* Subtle background highlight */
        }
        .definition {
            border-color: #4db6ac; /* Teal */
        }
        .example-box {
            border-color: #ffb74d; /* Orange */
        }
        .proof-box {
            border-color: #9575cd; /* Purple */
        }
        .exam-question {
            border-color: #66bb6a; /* Green */
        }
        .proof-box h3, .example-box h3, .exam-question h3 {
            margin-top: 0;
            color: #ffffff;
        }

        /* Code and Math Styling */
        code {
            background-color: #333;
            color: #f5f5f5;
            padding: 0.2em 0.4em;
            margin: 0;
            font-size: 90%;
            border-radius: 3px;
        }
        .katex-display {
            margin: 1em 0;
            overflow-x: auto;
            overflow-y: hidden;
            padding: 10px 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            body { padding: 10px; }
            .container { padding: 10px; }
            header h1 { font-size: 2rem; }
            .card h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>A Practical Guide to Statistical Estimators</h1>
            <p>Understanding how to evaluate and prove the properties of estimators.</p>
        </header>

        <main>
            <!-- Unbiasedness Section -->
            <section id="unbiasedness" class="card">
                <h2>1. Unbiasedness</h2>
                <p>An estimator is unbiased if its expected value (its long-run average) is equal to the true population parameter it's trying to estimate.</p>
                <div class="definition">An estimator $\hat{\theta}$ is an <strong>unbiased estimator</strong> of a parameter $\theta$ if its expected value is equal to $\theta$. $$ E[\hat{\theta}] = \theta $$</div>
                
                <div class="exam-question">
                    <h3>Exam Question 1: Unbiasedness of Sample Variance</h3>
                    <p>Let $X_1, \dots, X_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2$. Show that the sample variance $S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2$ is an unbiased estimator for $\sigma^2$.</p>
                    <p><strong>Solution:</strong> We need to show that $E[S^2] = \sigma^2$. A useful identity is $\sum(X_i - \bar{X})^2 = \sum X_i^2 - n\bar{X}^2$. Taking the expectation:
                    $$ E\left[\sum(X_i - \bar{X})^2\right] = \sum E[X_i^2] - nE[\bar{X}^2] $$
                    Using $E[Y^2] = Var(Y) + (E[Y])^2$, we have $E[X_i^2] = \sigma^2 + \mu^2$ and $E[\bar{X}^2] = \frac{\sigma^2}{n} + \mu^2$. Substituting these in:
                    $$ E\left[\sum(X_i - \bar{X})^2\right] = \sum(\sigma^2 + \mu^2) - n\left(\frac{\sigma^2}{n} + \mu^2\right) = (n\sigma^2 + n\mu^2) - (\sigma^2 + n\mu^2) = (n-1)\sigma^2 $$
                    Finally, $E[S^2] = \frac{1}{n-1} E\left[\sum(X_i - \bar{X})^2\right] = \frac{1}{n-1}(n-1)\sigma^2 = \sigma^2$. Thus, $S^2$ is unbiased.</p>
                </div>

                <div class="exam-question">
                    <h3>Exam Question 2: Estimator for a Uniform Distribution</h3>
                    <p>Let $X_1, \dots, X_n$ be a random sample from a Uniform(0, $\theta$) distribution. The mean of this distribution is $E[X] = \theta/2$. Consider the estimator $\hat{\theta} = 2\bar{X}$. Is this an unbiased estimator for $\theta$?</p>
                    <p><strong>Solution:</strong> We calculate the expected value of the estimator $\hat{\theta}$.</p>
                    $$ E[\hat{\theta}] = E[2\bar{X}] = 2E[\bar{X}] $$
                    We know that $\bar{X}$ is an unbiased estimator for the population mean, so $E[\bar{X}] = E[X] = \theta/2$.
                    $$ E[\hat{\theta}] = 2 \left( \frac{\theta}{2} \right) = \theta $$
                    <strong>Conclusion:</strong> Since $E[\hat{\theta}] = \theta$, the estimator $\hat{\theta} = 2\bar{X}$ is an unbiased estimator for $\theta$.
                </div>
            </section>

            <!-- Consistency Section -->
            <section id="consistency" class="card">
                <h2>2. Consistency</h2>
                <p>A consistent estimator gets more accurate as you collect more data, converging to the true value of the parameter as the sample size ($n$) grows to infinity.</p>
                <div class="definition">An estimator $\hat{\theta}_n$ is <strong>consistent</strong> if for any small $\epsilon > 0$, $\lim_{n \to \infty} P(|\hat{\theta}_n - \theta| < \epsilon) = 1$. A sufficient condition is that its bias and variance both go to zero as $n \to \infty$.</div>

                <div class="exam-question">
                    <h3>Exam Question 1: An Inconsistent Estimator</h3>
                    <p>Let $X_1, \dots, X_n$ be a random sample with mean $\mu$ and variance $\sigma^2$. Is the estimator $\hat{\mu} = \frac{X_1 + X_n}{2}$ consistent for $\mu$?</p>
                    <p><strong>Solution:</strong> We check the two conditions.
                    <strong>1. Bias:</strong> $E[\hat{\mu}] = E[\frac{X_1 + X_n}{2}] = \frac{1}{2}(\mu + \mu) = \mu$. The bias is 0.
                    <strong>2. Variance:</strong> $Var(\hat{\mu}) = Var(\frac{X_1 + X_n}{2}) = \frac{1}{4}(Var(X_1) + Var(X_n)) = \frac{1}{4}(\sigma^2 + \sigma^2) = \frac{\sigma^2}{2}$.
                    The limit is $\lim_{n \to \infty} Var(\hat{\mu}) = \frac{\sigma^2}{2} \neq 0$.
                    <strong>Conclusion:</strong> Since the variance does not approach 0, the estimator is <strong>not consistent</strong>.</p>
                </div>
                
                <div class="exam-question">
                    <h3>Exam Question 2: Consistency of Sample Variance</h3>
                    <p>Show that the sample variance $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$ is a consistent estimator for $\sigma^2$, assuming the population is Normally distributed.</p>
                    <p><strong>Solution:</strong> We check the two conditions.
                    <strong>1. Bias:</strong> From the previous section, we know $E[S^2] = \sigma^2$, so the bias is 0 for all $n$.
                    <strong>2. Variance:</strong> For a sample from a Normal distribution, the variance of the sample variance is given by $Var(S^2) = \frac{2\sigma^4}{n-1}$.
                    Now, we take the limit as $n \to \infty$:
                    $$ \lim_{n \to \infty} Var(S^2) = \lim_{n \to \infty} \frac{2\sigma^4}{n-1} = 0 $$
                    <strong>Conclusion:</strong> Since the estimator is unbiased and its variance approaches 0, $S^2$ is a <strong>consistent estimator</strong> for $\sigma^2$.</p>
                </div>
            </section>

            <!-- Efficiency Section -->
            <section id="efficiency" class="card">
                <h2>3. Efficiency</h2>
                <p>If we have two unbiased estimators, the one with the smaller variance is more <strong>efficient</strong>. It's more precise.</p>
                <div class="definition">Given two unbiased estimators, $\hat{\theta}_1$ and $\hat{\theta}_2$, the <strong>relative efficiency</strong> of $\hat{\theta}_1$ to $\hat{\theta}_2$ is $Eff(\hat{\theta}_1, \hat{\theta}_2) = \frac{Var(\hat{\theta}_2)}{Var(\hat{\theta}_1)}$. If $> 1$, $\hat{\theta}_1$ is more efficient.</div>

                <div class="exam-question">
                    <h3>Exam Question 1: Comparing Two Estimators</h3>
                    <p>Let $X_1, X_2, X_3$ be a random sample from a population with mean $\mu$ and variance $\sigma^2$. Consider two estimators for $\mu$: $\hat{\mu}_1 = \frac{X_1+X_2+X_3}{3}$ and $\hat{\mu}_2 = \frac{X_1+2X_2+X_3}{4}$. Which is more efficient?</p>
                    <p><strong>Solution:</strong> First, confirm they are unbiased.
                    $E[\hat{\mu}_1] = \frac{1}{3}(E[X_1]+E[X_2]+E[X_3]) = \frac{3\mu}{3} = \mu$. Unbiased.
                    $E[\hat{\mu}_2] = \frac{1}{4}(E[X_1]+2E[X_2]+E[X_3]) = \frac{1}{4}(\mu+2\mu+\mu) = \frac{4\mu}{4} = \mu$. Unbiased.
                    Now, find their variances.
                    $Var(\hat{\mu}_1) = \frac{1}{9}(Var(X_1)+Var(X_2)+Var(X_3)) = \frac{3\sigma^2}{9} = \frac{\sigma^2}{3}$.
                    $Var(\hat{\mu}_2) = \frac{1}{16}(Var(X_1)+4Var(X_2)+Var(X_3)) = \frac{1}{16}(\sigma^2+4\sigma^2+\sigma^2) = \frac{6\sigma^2}{16} = \frac{3\sigma^2}{8}$.
                    Comparing the variances: $\frac{\sigma^2}{3} \approx 0.333\sigma^2$ and $\frac{3\sigma^2}{8} = 0.375\sigma^2$.
                    Since $Var(\hat{\mu}_1) < Var(\hat{\mu}_2)$, the estimator $\hat{\mu}_1 = \bar{X}$ is <strong>more efficient</strong>.</p>
                </div>
                <div class="exam-question">
                    <h3>Exam Question 2: Mean vs. Median</h3>
                    <p>For a large sample of size $n=100$ from a Normal distribution with variance $\sigma^2=25$, calculate the relative efficiency of the sample mean ($\bar{X}$) with respect to the sample median ($\tilde{X}$).</p>
                    <p><strong>Solution:</strong> We need the variances of the two estimators.
                    $Var(\bar{X}) = \frac{\sigma^2}{n} = \frac{25}{100} = 0.25$.
                    For large samples from a Normal distribution, the variance of the median is approximately $Var(\tilde{X}) \approx \frac{\pi\sigma^2}{2n} = \frac{\pi(25)}{2(100)} = \frac{25\pi}{200} = \frac{\pi}{8} \approx 0.3927$.
                    The relative efficiency is:
                    $$ Eff(\bar{X}, \tilde{X}) = \frac{Var(\tilde{X})}{Var(\bar{X})} = \frac{\pi/8}{1/4} = \frac{\pi}{8} \cdot 4 = \frac{\pi}{2} \approx 1.57 $$
                    <strong>Conclusion:</strong> The sample mean is approximately 57% more efficient than the sample median for estimating the center of a Normal distribution.</p>
                </div>
            </section>
            
            <!-- Sufficiency Section -->
            <section id="sufficiency" class="card">
                <h2>4. Sufficiency & The Factorization Theorem</h2>
                <p>A sufficient statistic squeezes every drop of information about the parameter out of the sample. The <strong>Neyman-Fisher Factorization Theorem</strong> is our tool to find one.</p>
                <div class="definition"><strong>Factorization Theorem:</strong> A statistic $T(\mathbf{X})$ is sufficient for $\theta$ if the joint PDF/PMF can be factored as $f(\mathbf{x}|\theta) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x})$.</div>
                <div class="exam-question">
                    <h3>Exam Question 1: Sufficiency for a Poisson Distribution</h3>
                    <p>Let $X_1, \dots, X_n$ be a random sample from a Poisson($\lambda$) distribution. Find a sufficient statistic for $\lambda$.</p>
                    <p><strong>Solution:</strong> The joint PMF is $f(\mathbf{x}|\lambda) = \prod \frac{e^{-\lambda}\lambda^{x_i}}{x_i!} = \frac{e^{-n\lambda}\lambda^{\sum x_i}}{\prod x_i!}$.
                    We can factor this as:
                    $$ f(\mathbf{x} | \lambda) = \underbrace{(e^{-n\lambda} \lambda^{\sum x_i})}_{g(T(\mathbf{x}), \lambda)} \cdot \underbrace{\left(\frac{1}{\prod x_i!}\right)}_{h(\mathbf{x})} $$
                    The function $g$ depends on the data only through $T(\mathbf{x}) = \sum X_i$.
                    <strong>Conclusion:</strong> $T(\mathbf{X}) = \sum X_i$ is a sufficient statistic for $\lambda$.</p>
                </div>
                <div class="exam-question">
                    <h3>Exam Question 2: Sufficiency for a Normal Mean</h3>
                    <p>Let $X_1, \dots, X_n$ be a sample from a Normal distribution $N(\mu, 1)$ where the variance is known to be 1. Find a sufficient statistic for $\mu$.</p>
                    <p><strong>Solution:</strong> The joint PDF is $f(\mathbf{x}|\mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-\frac{(x_i-\mu)^2}{2}} = (\frac{1}{2\pi})^{n/2} e^{-\frac{1}{2}\sum(x_i-\mu)^2}$.
                    Let's expand the exponent: $\sum(x_i-\mu)^2 = \sum(x_i^2 - 2\mu x_i + \mu^2) = \sum x_i^2 - 2\mu\sum x_i + n\mu^2$.
                    Substitute this back into the PDF:
                    $$ f(\mathbf{x}|\mu) = (\frac{1}{2\pi})^{n/2} e^{-\frac{1}{2}(\sum x_i^2 - 2\mu\sum x_i + n\mu^2)} $$
                    $$ = \underbrace{(e^{\mu\sum x_i - \frac{n\mu^2}{2}})}_{g(T(\mathbf{x}), \mu)} \cdot \underbrace{((\frac{1}{2\pi})^{n/2} e^{-\frac{1}{2}\sum x_i^2})}_{h(\mathbf{x})} $$
                    The function $g$ depends on the data only through $T(\mathbf{x}) = \sum X_i$.
                    <strong>Conclusion:</strong> $T(\mathbf{X}) = \sum X_i$ (and by extension, $\bar{X}$) is a sufficient statistic for $\mu$.</p>
                </div>
            </section>

            <!-- Cramer-Rao Section -->
            <section id="cramer-rao" class="card">
                <h2>5. The Cramer-Rao Inequality</h2>
                <p>This theorem gives a lower bound on the variance of any unbiased estimator. An estimator that meets this bound is the most efficient possible.</p>
                <div class="definition"><strong>Cramer-Rao Lower Bound (CRLB):</strong> For an unbiased estimator $\hat{\theta}$, $Var(\hat{\theta}) \ge \frac{1}{I(\theta)}$, where $I(\theta)$ is the Fisher Information, $I(\theta) = -n \cdot E[ \frac{\partial^2}{\partial\theta^2} \ln f(X|\theta) ]$.</div>
                <div class="exam-question">
                    <h3>Exam Question 1: CRLB for an Exponential Distribution</h3>
                    <p>For a sample from an Exponential($\theta$) distribution, show that the sample mean $\bar{X}$ attains the CRLB for $\theta$.</p>
                    <p><strong>Solution:</strong> The PDF is $f(x|\theta) = \frac{1}{\theta}e^{-x/\theta}$.
                    <strong>1. Find CRLB:</strong> The log-likelihood is $\ln f(x|\theta) = -\ln\theta - x/\theta$. The second derivative is $\frac{\partial^2}{\partial\theta^2} \ln f = \frac{1}{\theta^2} - \frac{2x}{\theta^3}$.
                    Its expectation is $E[\dots] = \frac{1}{\theta^2} - \frac{2E[X]}{\theta^3} = \frac{1}{\theta^2} - \frac{2\theta}{\theta^3} = -\frac{1}{\theta^2}$.
                    The Fisher Information is $I(\theta) = -n(-\frac{1}{\theta^2}) = \frac{n}{\theta^2}$.
                    The CRLB is $\frac{1}{I(\theta)} = \frac{\theta^2}{n}$.
                    <strong>2. Check $\bar{X}$:</strong> $\bar{X}$ is unbiased for $\theta$. Its variance is $Var(\bar{X}) = \frac{Var(X)}{n} = \frac{\theta^2}{n}$.
                    <strong>Conclusion:</strong> Since $Var(\bar{X}) = \text{CRLB}$, $\bar{X}$ is the MVUE for $\theta$.</p>
                </div>
                <div class="exam-question">
                    <h3>Exam Question 2: CRLB for a Bernoulli trial</h3>
                    <p>For a sample from a Bernoulli($p$) distribution, find the CRLB for an unbiased estimator of $p$. Does the sample proportion $\hat{p}=\bar{X}$ attain it?</p>
                    <p><strong>Solution:</strong> The PMF is $f(x|p)=p^x(1-p)^{1-x}$.
                    <strong>1. Find CRLB:</strong> The log-likelihood is $\ln f(x|p) = x\ln p + (1-x)\ln(1-p)$.
                    The second derivative is $\frac{\partial^2}{\partial p^2} \ln f = -\frac{x}{p^2} - \frac{1-x}{(1-p)^2}$.
                    Its expectation is $E[\dots] = -\frac{E[X]}{p^2} - \frac{1-E[X]}{(1-p)^2} = -\frac{p}{p^2} - \frac{1-p}{(1-p)^2} = -\frac{1}{p} - \frac{1}{1-p} = -\frac{1}{p(1-p)}$.
                    The Fisher Information is $I(p) = -n(-\frac{1}{p(1-p)}) = \frac{n}{p(1-p)}$.
                    The CRLB is $\frac{1}{I(p)} = \frac{p(1-p)}{n}$.
                    <strong>2. Check $\hat{p}=\bar{X}$:</strong> $\hat{p}$ is unbiased for $p$. Its variance is $Var(\hat{p}) = Var(\bar{X}) = \frac{Var(X)}{n} = \frac{p(1-p)}{n}$.
                    <strong>Conclusion:</strong> Since $Var(\hat{p}) = \text{CRLB}$, the sample proportion is the MVUE for $p$.</p>
                </div>
            </section>

            <!-- MLE Section -->
            <section id="mle" class="card">
                <h2>6. Maximum Likelihood Estimation (MLE)</h2>
                <p>MLE finds the parameter value that makes the observed data most probable.</p>
                <div class="proof-box"><h3>How to Find the MLE</h3><ol><li>Write the Likelihood Function $L(\theta) = \prod f(x_i|\theta)$.</li><li>Take the Log-Likelihood $\ell(\theta) = \ln L(\theta)$.</li><li>Differentiate $\ell(\theta)$ with respect to $\theta$, set to 0, and solve.</li></ol></div>
                <div class="exam-question">
                    <h3>Exam Question 1: MLE for a Geometric Distribution</h3>
                    <p>For a sample from a Geometric($p$) distribution, find the MLE for $p$. The PMF is $f(x|p) = (1-p)^x p$.</p>
                    <p><strong>Solution:</strong> The likelihood is $L(p) = \prod (1-p)^{x_i} p = p^n (1-p)^{\sum x_i}$.
                    The log-likelihood is $\ell(p) = n\ln(p) + (\sum x_i)\ln(1-p)$.
                    Differentiating: $\frac{d\ell}{dp} = \frac{n}{p} - \frac{\sum x_i}{1-p} = 0$.
                    Solving for $p$ gives $n(1-p) = p\sum x_i \implies n = p(n+\sum x_i)$.
                    $$ \hat{p}_{MLE} = \frac{n}{n + \sum x_i} = \frac{1}{1 + \bar{x}} $$
                    </p>
                </div>
                <div class="exam-question">
                    <h3>Exam Question 2: MLE for a Normal Mean</h3>
                    <p>Let $X_1, \dots, X_n$ be a sample from $N(\mu, \sigma^2)$ where $\sigma^2$ is known. Find the MLE for $\mu$.</p>
                    <p><strong>Solution:</strong> The likelihood is $L(\mu) = \prod \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}$.
                    The log-likelihood is $\ell(\mu) = \ln L(\mu) = C - \frac{1}{2\sigma^2} \sum(x_i-\mu)^2$, where C is a constant not depending on $\mu$.
                    To maximize $\ell(\mu)$, we must minimize the sum of squares $\sum(x_i-\mu)^2$.
                    Differentiating the sum with respect to $\mu$ and setting to 0:
                    $$ \frac{d}{d\mu} \sum(x_i-\mu)^2 = \sum -2(x_i-\mu) = -2(\sum x_i - n\mu) = 0 $$
                    This implies $\sum x_i - n\mu = 0$, which gives $\mu = \frac{\sum x_i}{n}$.
                    <strong>Conclusion:</strong> The MLE for the mean $\mu$ is the sample mean, $\hat{\mu}_{MLE} = \bar{X}$.</p>
                </div>
            </section>

        </main>
    </div>

</body>
</html>
