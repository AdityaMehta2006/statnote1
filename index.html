<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StatNotes: Theory of Estimation</title>
    
    <!-- Google Fonts: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

    <!-- KaTeX for LaTeX Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0KOVEMQNogPSesLpvJxifG/T8LC5e82CE0CYfownAIMPxlVdFsVdk" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ],
                throwOnError : false
            });
        });
    </script>

    <style>
        /* Base Styling */
        :root {
            --bg-dark: #121212;
            --bg-card: #1e1e1e;
            --text-light: #e0e0e0;
            --text-white: #ffffff;
            --text-muted: #a0a0a0;
            --border-color: #333;
            --accent-teal: #4db6ac;
            --accent-green: #66bb6a;
            --accent-orange: #ffb74d;
            --accent-purple: #9575cd;
        }
        html {
            scroll-behavior: smooth;
        }
        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-dark);
            color: var(--text-light);
            line-height: 1.7;
        }

        /* Layout */
        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header */
        header {
            text-align: center;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        header h1 {
            font-size: 2.8rem;
            color: var(--text-white);
            font-weight: 700;
        }
        header p {
            font-size: 1.2rem;
            color: var(--text-muted);
        }

        /* Tab Navigation */
        .tab-nav {
            display: flex;
            flex-wrap: wrap;
            border-bottom: 2px solid var(--border-color);
            margin-bottom: 30px;
        }
        .tab-button {
            padding: 12px 20px;
            cursor: pointer;
            background: none;
            border: none;
            color: var(--text-muted);
            font-size: 1rem;
            font-weight: 500;
            border-bottom: 3px solid transparent;
            transition: color 0.2s, border-color 0.2s;
        }
        .tab-button:hover {
            color: var(--text-white);
        }
        .tab-button.active {
            color: var(--accent-teal);
            border-bottom-color: var(--accent-teal);
        }

        /* Tab Content */
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }

        /* Section Card Styling */
        .card {
            background-color: var(--bg-card);
            border-radius: 12px;
            padding: 30px;
            margin-bottom: 20px;
            border: 1px solid var(--border-color);
        }
        .card h2 {
            font-size: 2.2rem;
            color: var(--text-white);
            border-bottom: 2px solid var(--accent-teal);
            padding-bottom: 10px;
            margin-top: 0;
        }
        .card h3 {
            font-size: 1.3rem;
            color: #f5f5f5;
            margin-top: 25px;
        }

        /* Info Boxes */
        .definition, .example-box, .proof-box, .exam-question {
            padding: 20px;
            margin-top: 20px;
            border-radius: 8px;
            border-left: 5px solid;
            background-color: rgba(255, 255, 255, 0.05);
        }
        .definition { border-color: var(--accent-teal); }
        .example-box { border-color: var(--accent-orange); }
        .proof-box { border-color: var(--accent-purple); margin-bottom: 25px; }
        .exam-question { 
            border-color: var(--accent-green); 
            margin-bottom: 35px; /* Increased space between problems */
        }
        .proof-box h3, .exam-question h3 {
            margin-top: 0;
            color: var(--text-white);
            margin-bottom: 1rem;
        }
        .exam-question .solution {
            margin-top: 1.5rem; /* Added space between question and solution */
        }
        .exam-question p:not(:last-child), .proof-box p:not(:last-child) {
             margin-bottom: 1rem;
        }
        .exam-question ul {
            list-style-position: inside;
            padding-left: 0;
        }

        /* Code and Math */
        code {
            background-color: #333;
            color: #f5f5f5;
            padding: 0.2em 0.4em;
            margin: 0;
            font-size: 90%;
            border-radius: 3px;
        }
        .katex-display {
            margin: 1em 0;
            overflow-x: auto;
            padding: 10px 0;
        }

        /* Responsive Design for Mobile */
        @media (max-width: 768px) {
            .container {
                padding: 15px;
            }
            header h1 { 
                font-size: 2.2rem; 
            }
            header p {
                font-size: 1.1rem;
            }
            .card {
                padding: 20px;
            }
            .card h2 { 
                font-size: 1.8rem; 
            }
            .card h3 {
                font-size: 1.15rem;
            }
            .tab-button { 
                padding: 10px 15px; 
                font-size: 0.9rem;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>StatNotes</h1>
            <p>Theory of Estimation</p>
        </header>

        <nav class="tab-nav">
            <button class="tab-button active" onclick="openTab(event, 'foundations')">Foundations</button>
            <button class="tab-button" onclick="openTab(event, 'properties')">Estimator Properties</button>
            <button class="tab-button" onclick="openTab(event, 'methods')">Estimation Methods</button>
            <button class="tab-button" onclick="openTab(event, 'proofs')">Key Proofs</button>
            <button class="tab-button" onclick="openTab(event, 'problems')">Important Problems</button>
        </nav>

        <main>
            <!-- Foundations Tab -->
            <div id="foundations" class="tab-content active">
                <section class="card">
                    <h2>1. Population and Sample</h2>
                    <p>The first step in any statistical study is to define the groups you're interested in. This is where the concepts of population and sample come in.</p>
                    <div class="definition">
                        <ul>
                            <li>A <strong>Population</strong> is the entire collection of individuals, items, or events that you want to make inferences about. It's the "whole group".</li>
                            <li>A <strong>Sample</strong> is a subset of the population that is selected for analysis. Since studying an entire population is often impractical or impossible, we use a sample to draw conclusions about it.</li>
                        </ul>
                    </div>
                </section>
                <section class="card">
                    <h2>2. Parameter, Statistic, & Estimate</h2>
                    <p>These terms form the core vocabulary of statistical inference. They describe numerical characteristics of groups and the process of estimation.</p>
                    <div class="definition">
                        <ul>
                            <li>A <strong>Parameter</strong> is a numerical value describing a characteristic of a <strong>population</strong>. We often use Greek letters (e.g., $\mu, \sigma, \theta, P$). The true value of a parameter is typically unknown and fixed.</li>
                            <li>The <strong>Parameter Space</strong> (often denoted by $\Omega$) is the set of all possible values that a parameter $\theta$ can take. For example, the parameter $p$ for a Bernoulli trial has a parameter space of $\Omega = [0, 1]$.</li>
                            <li>An <strong>Estimator</strong> is a rule or formula (a function of the sample data) used to estimate an unknown population parameter. It is a random variable because it depends on the random sample. We use symbols like $\hat{\theta}$ or $\bar{X}$.</li>
                            <li>An <strong>Estimate</strong> is the specific numerical value an estimator takes for a particular sample. If you collect a sample and calculate the mean, the number you get (e.g., 5.2) is an estimate.</li>
                        </ul>
                    </div>
                     <div class="example-box">
                        <h3>Example: Tying It All Together</h3>
                        <p>You want to know the average height of all adult women in a city.</p>
                        <ul>
                            <li>The true average height is the unknown <strong>parameter</strong>, $\mu$. The <strong>parameter space</strong> for $\mu$ would be all positive real numbers, $\Omega = (0, \infty)$.</li>
                            <li>The rule "sum all the sample heights and divide by the sample size" is the <strong>estimator</strong>, denoted by the formula $\bar{X} = \frac{1}{n}\sum X_i$.</li>
                            <li>You take a sample of 100 women and find their average height is 165 cm. The number 165 is the <strong>estimate</strong>.</li>
                        </ul>
                    </div>
                </section>
                <section class="card">
                    <h2>3. Sampling and Its Errors</h2>
                    <p>How you choose your sample is critical. Good methods ensure the sample is representative, but no sample is ever a perfect mirror of the population.</p>
                    <div class="definition">
                        <p><strong>Simple Random Sampling (SRS):</strong> Every member of the population has an equal chance of being selected.</p>
                        <p><strong>Stratified Sampling:</strong> The population is divided into subgroups ('strata'), and random samples are taken from each.</p>
                        <p><strong>Sampling Error:</strong> The natural difference between a sample statistic and the population parameter due to chance. It is not a mistake. Increasing sample size reduces sampling error.</p>
                        <p><strong>Non-Sampling Error:</strong> Errors from other sources, like data entry mistakes, biased questions, or non-response. These are avoidable with careful study design.</p>
                    </div>
                </section>
                <section class="card">
                    <h2>4. Sampling Distribution and Standard Error</h2>
                    <p>If we took many different samples from the same population, we would get a different estimate each time. The distribution of all these possible estimates is key to understanding an estimator's precision.</p>
                    <div class="definition">
                        <p>A <strong>Sampling Distribution</strong> is the probability distribution of a statistic (the estimator) obtained from a large number of samples of the same size.</p>
                        <p>The <strong>Standard Error (SE)</strong> is the standard deviation of the sampling distribution. It measures the typical amount by which an estimate differs from the population parameter. A smaller SE means a more precise estimator.</p>
                    </div>
                </section>
            </div>

            <!-- Properties Tab -->
            <div id="properties" class="tab-content">
                <section class="card">
                    <h2>5. Unbiasedness</h2>
                    <p>Unbiasedness is a highly desirable property for an estimator. It means that the estimator does not systematically over or underestimate the parameter. While any single estimate might be off, an unbiased estimator is correct "on average" over many repeated samples.</p>
                    <div class="definition">An estimator $\hat{\theta}$ is an <strong>unbiased estimator</strong> of a parameter $\theta$ if its expected value is equal to $\theta$. $$ E[\hat{\theta}] = \theta $$ The <strong>bias</strong> is defined as $Bias(\hat{\theta}) = E[\hat{\theta}] - \theta$. For an unbiased estimator, the bias is 0.</div>
                    <div class="example-box">
                        <h3>Is Unbiasedness Always Best?</h3>
                        <p>Not necessarily. Sometimes a slightly biased estimator might be preferred if its variance is much smaller than that of any unbiased estimator. This trade-off is measured by the Mean Squared Error (MSE), where $MSE(\hat{\theta}) = Var(\hat{\theta}) + (Bias(\hat{\theta}))^2$. An estimator with a lower MSE is often considered better, even if it has a small bias.</p>
                    </div>
                </section>
                <section class="card">
                    <h2>6. Consistency</h2>
                    <p>Consistency is an asymptotic property, meaning it describes the estimator's behavior as the sample size grows infinitely large. It ensures that with enough data, our estimate will almost certainly be close to the true parameter value.</p>
                    <div class="definition">An estimator $\hat{\theta}_n$ is <strong>consistent</strong> if it converges in probability to the true parameter $\theta$. Formally, for any small $\epsilon > 0$: $$ \lim_{n \to \infty} P(|\hat{\theta}_n - \theta| < \epsilon) = 1 $$</div>
                    <div class="proof-box">
                        <h3>Sufficient Conditions for Consistency</h3>
                        <p>A common method to prove consistency is to show that the Mean Squared Error (MSE) of the estimator approaches zero as $n \to \infty$. This is equivalent to showing two conditions:</p>
                        <ol>
                            <li>The bias approaches zero: $\lim_{n \to \infty} Bias(\hat{\theta}_n) = 0$ (The estimator is asymptotically unbiased).</li>
                            <li>The variance approaches zero: $\lim_{n \to \infty} Var(\hat{\theta}_n) = 0$.</li>
                        </ol>
                    </div>
                </section>
                <section class="card">
                    <h2>7. Efficiency</h2>
                    <p>If we have two unbiased estimators, the one with the smaller variance is more <strong>efficient</strong>. It's more precise because its estimates are less spread out and closer to the true parameter value on average.</p>
                    <div class="definition">
                        <p>Given two unbiased estimators, $\hat{\theta}_1$ and $\hat{\theta}_2$, the <strong>relative efficiency</strong> of $\hat{\theta}_1$ to $\hat{\theta}_2$ is $Eff(\hat{\theta}_1, \hat{\theta}_2) = \frac{Var(\hat{\theta}_2)}{Var(\hat{\theta}_1)}$. If this value is greater than 1, $\hat{\theta}_1$ is more efficient.</p>
                        <p>An unbiased estimator is called a <strong>Minimum Variance Unbiased Estimator (MVUE)</strong> if it has the lowest possible variance among all unbiased estimators.</p>
                    </div>
                </section>
                <section class="card">
                    <h2>8. Sufficiency</h2>
                    <p>A sufficient statistic is a function of the sample data that captures all the relevant information about the unknown parameter $\theta$. Once a sufficient statistic is known, the original data provides no further information. This is a powerful data reduction principle.</p>
                    <div class="definition"><strong>Neyman-Fisher Factorization Theorem:</strong> A statistic $T(\mathbf{X})$ is sufficient for $\theta$ if and only if the joint PDF/PMF, $f(\mathbf{x}|\theta)$, can be factored into two non-negative functions:
                    $$ f(\mathbf{x}|\theta) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x}) $$
                    where $g$ depends on the data $\mathbf{x}$ only through the value of the statistic $T(\mathbf{x})$, and $h(\mathbf{x})$ does not depend on the parameter $\theta$.</div>
                    <div class="example-box">
                        <h3>Example: Sufficiency for a Bernoulli Trial</h3>
                        <p>Let $X_1, \dots, X_n$ be a random sample from a Bernoulli($p$) distribution. The joint PMF is:
                        $$ f(\mathbf{x}|p) = \prod p^{x_i}(1-p)^{1-x_i} = p^{\sum x_i}(1-p)^{n-\sum x_i} $$
                        Let $T(\mathbf{X}) = \sum X_i$. We can factor the PMF as:
                        $$ f(\mathbf{x}|p) = \underbrace{p^{T(\mathbf{x})}(1-p)^{n-T(\mathbf{x})}}_{g(T(\mathbf{x}), p)} \cdot \underbrace{1}_{h(\mathbf{x})} $$
                        Since $g$ depends on the data only through the sum $\sum X_i$ and $h(\mathbf{x})=1$ does not depend on $p$, the statistic $T(\mathbf{X}) = \sum X_i$ (the total number of successes) is sufficient for $p$.</p>
                    </div>
                </section>
            </div>

            <!-- Methods Tab -->
            <div id="methods" class="tab-content">
                <section class="card">
                    <h2>9. The Cramer-Rao Inequality</h2>
                    <p>This theorem is a cornerstone of estimation theory. It provides a theoretical "speed limit" for how good an unbiased estimator can be by establishing a lower bound on its variance. An estimator whose variance reaches this bound is called a Minimum Variance Unbiased Estimator (MVUE) and is the most efficient possible.</p>
                    <div class="definition">
                        <h3>The Lower Bound</h3>
                        <p>Let $X_1, \dots, X_n$ be a random sample from a distribution with PDF/PMF $f(x|\theta)$. Under certain regularity conditions, the variance of any unbiased estimator $\hat{\theta}$ of $\theta$ must satisfy:</p>
                        $$ Var(\hat{\theta}) \ge \frac{1}{I(\theta)} $$
                        The term on the right is the <strong>Cramer-Rao Lower Bound (CRLB)</strong>. An estimator that achieves this bound is an MVUE.
                    </div>
                    <div class="proof-box">
                        <h3>Fisher Information and Regularity Conditions</h3>
                        <p>The quantity $I(\theta)$ is the <strong>Fisher Information</strong>, which measures the amount of information the data provides about the parameter $\theta$. A "peaked" likelihood function has high information, while a "flat" one has low information. It has two equivalent forms:</p>
                        $$ I(\theta) = n \cdot E\left[ \left( \frac{\partial}{\partial\theta} \ln f(X|\theta) \right)^2 \right] = -n \cdot E\left[ \frac{\partial^2}{\partial\theta^2} \ln f(X|\theta) \right] $$
                        <p>The CRLB holds under these key <strong>regularity conditions</strong>:
                        <ul>
                            <li>The parameter space for $\theta$ is an open interval.</li>
                            <li>The support of the distribution (the set of possible values for X) does not depend on $\theta$.</li>
                            <li>The derivative of the log-likelihood with respect to $\theta$ exists and is well-behaved.</li>
                        </ul>
                        </p>
                    </div>
                </section>
                <section class="card">
                    <h2>10. Maximum Likelihood Estimation (MLE)</h2>
                    <p>MLE is a very popular method for finding estimators. The core idea is simple: given the data you collected, what value of the parameter $\theta$ would make that data the <strong>most likely</strong> to have occurred? We find the parameter value that maximizes the likelihood function.</p>
                    <div class="proof-box">
                        <h3>How to Find the MLE: A Step-by-Step Guide</h3>
                        <p><strong>Step 1: Write the Likelihood Function, $L(\theta)$</strong><br>
                        The likelihood is the joint PDF/PMF of your sample. For an i.i.d. sample, this is the product of the individual PDFs/PMFs: $L(\theta) = \prod_{i=1}^n f(x_i|\theta)$.</p>
                        <p><strong>Step 2: Take the Log-Likelihood, $\ell(\theta)$</strong><br>
                        Maximizing the log-likelihood is easier because it turns products into sums: $\ell(\theta) = \ln L(\theta) = \sum_{i=1}^n \ln f(x_i|\theta)$. The value of $\theta$ that maximizes $L(\theta)$ also maximizes $\ell(\theta)$.</p>
                        <p><strong>Step 3: Differentiate and Solve (The "Score Equation")</strong><br>
                        Calculate the derivative of the log-likelihood with respect to $\theta$, set it to zero, and solve for $\theta$. This equation is called the score equation.
                        $$ \frac{d\ell}{d\theta} = 0 $$
                        The solution is your candidate for the MLE.</p>
                        <p><strong>Step 4: Verify it's a Maximum</strong><br>
                        Check that the second derivative is negative at your candidate value: $\frac{d^2\ell}{d\theta^2} < 0$. This confirms it's a local maximum.</p>
                        <p><strong>Important Exception:</strong> If the support of the distribution depends on $\theta$ (e.g., Uniform(0, $\theta$)), the derivative method may not work. In these cases, you must inspect the likelihood function directly to find the value of $\theta$ that maximizes it, which often occurs at a boundary of the parameter space.</p>
                    </div>
                    <div class="proof-box">
                        <h3>Properties of Maximum Likelihood Estimators</h3>
                        <p>Under general regularity conditions, MLEs have several desirable large-sample (asymptotic) properties:</p>
                        <ul>
                            <li><strong>Consistency:</strong> $\hat{\theta}_{MLE}$ is a consistent estimator of $\theta$.</li>
                            <li><strong>Asymptotic Normality:</strong> The distribution of $\hat{\theta}_{MLE}$ approaches a normal distribution as the sample size increases.</li>
                            <li><strong>Asymptotic Efficiency:</strong> For large samples, no other estimator has a smaller variance. It achieves the Cramer-Rao Lower Bound.</li>
                            <li><strong>Invariance Property:</strong> If $\hat{\theta}_{MLE}$ is the MLE of $\theta$, then for any function $g$, the MLE of $g(\theta)$ is $g(\hat{\theta}_{MLE})$. For example, the MLE of $\theta^2$ is $(\hat{\theta}_{MLE})^2$.</li>
                        </ul>
                    </div>
                </section>
            </div>

            <!-- Proofs Tab -->
            <div id="proofs" class="tab-content">
                <section class="card">
                    <h2>Key Proofs in Estimation Theory</h2>
                    <div class="proof-box">
                        <h3>Proof: Standard Error of the Sample Mean</h3>
                        <p><strong>Goal:</strong> To prove that the standard deviation of the sample mean $\bar{X}$, known as the standard error, is $SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}$.</p>
                        <p>Let $X_1, X_2, \dots, X_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2$. The sample mean is $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$. We want to find the standard deviation of $\bar{X}$, which is the square root of its variance.</p>
                        <p>First, let's find the variance of $\bar{X}$:
                        $$ Var(\bar{X}) = Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) $$
                        Using the properties of variance, we can pull the constant $\frac{1}{n}$ out, but it must be squared:
                        $$ Var(\bar{X}) = \frac{1}{n^2} Var\left(\sum_{i=1}^n X_i\right) $$
                        Since the observations in a random sample are independent, the variance of the sum is the sum of the variances:
                        $$ Var(\bar{X}) = \frac{1}{n^2} \left(\sum_{i=1}^n Var(X_i)\right) $$
                        We know that the variance of each individual observation $X_i$ is the population variance, $\sigma^2$:
                        $$ Var(\bar{X}) = \frac{1}{n^2} (Var(X_1) + Var(X_2) + \dots + Var(X_n)) = \frac{1}{n^2} (\sigma^2 + \sigma^2 + \dots + \sigma^2) = \frac{1}{n^2} (n\sigma^2) $$
                        $$ Var(\bar{X}) = \frac{\sigma^2}{n} $$
                        The standard error is the square root of the variance:
                        $$ SE(\bar{X}) = \sqrt{Var(\bar{X})} = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}} $$
                        This completes the proof.</p>
                    </div>
                    <div class="proof-box">
                        <h3>Proof: Unbiasedness of Sample Variance $S^2$</h3>
                        <p><strong>Goal:</strong> To prove that the sample variance $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$ is an unbiased estimator for the population variance $\sigma^2$. That is, $E[S^2] = \sigma^2$.</p>
                        <p>We start by expanding the sum of squares term:
                        $$ \sum(X_i - \bar{X})^2 = \sum(X_i^2 - 2X_i\bar{X} + \bar{X}^2) = \sum X_i^2 - 2\bar{X}\sum X_i + \sum \bar{X}^2 $$
                        Since $\bar{X} = \frac{\sum X_i}{n}$, we have $\sum X_i = n\bar{X}$. Substituting this in:
                        $$ \sum(X_i - \bar{X})^2 = \sum X_i^2 - 2\bar{X}(n\bar{X}) + n\bar{X}^2 = \sum X_i^2 - 2n\bar{X}^2 + n\bar{X}^2 = \sum X_i^2 - n\bar{X}^2 $$
                        Now, we take the expectation of this expression:
                        $$ E\left[\sum(X_i - \bar{X})^2\right] = E\left[\sum X_i^2 - n\bar{X}^2\right] = \sum E[X_i^2] - nE[\bar{X}^2] $$
                        We use the identity $Var(Y) = E[Y^2] - (E[Y])^2$, which implies $E[Y^2] = Var(Y) + (E[Y])^2$.
                        <br>For each $X_i$, $E[X_i^2] = Var(X_i) + (E[X_i])^2 = \sigma^2 + \mu^2$.
                        <br>For $\bar{X}$, $E[\bar{X}^2] = Var(\bar{X}) + (E[\bar{X}])^2 = \frac{\sigma^2}{n} + \mu^2$.
                        <br>Substituting these back into the expected value equation:
                        $$ E\left[\sum(X_i - \bar{X})^2\right] = \sum(\sigma^2 + \mu^2) - n\left(\frac{\sigma^2}{n} + \mu^2\right) $$
                        $$ = (n\sigma^2 + n\mu^2) - (\sigma^2 + n\mu^2) = n\sigma^2 - \sigma^2 = (n-1)\sigma^2 $$
                        Finally, we find the expectation of $S^2$:
                        $$ E[S^2] = E\left[\frac{1}{n-1}\sum(X_i - \bar{X})^2\right] = \frac{1}{n-1}E\left[\sum(X_i - \bar{X})^2\right] = \frac{1}{n-1}(n-1)\sigma^2 = \sigma^2 $$
                        This shows that $S^2$ is an unbiased estimator of $\sigma^2$. The division by $n-1$ is what corrects the bias.</p>
                    </div>
                     <div class="proof-box">
                        <h3>Proof: Invariance Property of Consistent Estimators</h3>
                        <p><strong>Goal:</strong> To show that if $\hat{\theta}_n$ is a consistent estimator for $\theta$ and $g$ is a function that is continuous at $\theta$, then $g(\hat{\theta}_n)$ is a consistent estimator for $g(\theta)$.</p>
                        <p>By definition, the consistency of $\hat{\theta}_n$ means that for any small $\delta > 0$,
                        $$ \lim_{n \to \infty} P(|\hat{\theta}_n - \theta| < \delta) = 1 $$
                        This means that as the sample size grows, the estimator $\hat{\theta}_n$ gets arbitrarily close to the true value $\theta$.
                        <br>By the definition of continuity of a function $g$ at a point $\theta$, for any small $\epsilon > 0$, there exists a $\delta > 0$ such that if $|\hat{\theta}_n - \theta| < \delta$, then it must be true that $|g(\hat{\theta}_n) - g(\theta)| < \epsilon$.
                        <br>In other words, if the input to the function is close to $\theta$, the output is close to $g(\theta)$.
                        <br>Since we know that the probability of $|\hat{\theta}_n - \theta| < \delta$ approaches 1, it follows that the probability of the consequence, $|g(\hat{\theta}_n) - g(\theta)| < \epsilon$, must also approach 1.
                        $$ \lim_{n \to \infty} P(|g(\hat{\theta}_n) - g(\theta)| < \epsilon) = 1 $$
                        This is precisely the definition of consistency for the estimator $g(\hat{\theta}_n)$. Therefore, $g(\hat{\theta}_n)$ is a consistent estimator for $g(\theta)$.</p>
                    </div>
                </section>
            </div>
            
            <!-- Problems Tab -->
            <div id="problems" class="tab-content">
                 <section class="card">
                    <h2>11. Practice Problems</h2>
                    <p>Test your understanding with this comprehensive set of problems.</p>
                    
                    <div class="exam-question">
                        <h3>Problem 1: Consistency of a Bernoulli Function</h3>
                        <p>If $X_1, \dots, X_n$ are random observations on a Bernoulli variate $X$ (value 1 with probability $p$), show that $\bar{X}(1-\bar{X})$ is a consistent estimator of $p(1-p)$.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> First, we show $\bar{X}$ is a consistent estimator for $p$. For a Bernoulli trial, $E[X]=p$ and $Var(X)=p(1-p)$.
                            <br>1. Check Bias: $E[\bar{X}] = E[\frac{1}{n}\sum X_i] = \frac{1}{n}(np) = p$. The bias is 0.
                            <br>2. Check Variance: $Var(\bar{X}) = \frac{1}{n^2}\sum Var(X_i) = \frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}$.
                            <br>Since $Bias(\bar{X})=0$ and $Var(\bar{X}) \to 0$ as $n \to \infty$, $\bar{X}$ is a consistent estimator for $p$.
                            <br>By the Invariance Property of Consistent Estimators, if $g$ is a continuous function, then $g(\bar{X})$ is a consistent estimator for $g(p)$. Let $g(p) = p(1-p)$. This is a continuous polynomial function. Therefore, $g(\bar{X}) = \bar{X}(1-\bar{X})$ is a consistent estimator for $p(1-p)$.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 2: Efficiency of Linear Estimators</h3>
                        <p>Let $X_1, X_2, X_3$ be a random sample of size 3 from a population with mean $\mu$ and variance $\sigma^2$. Consider three estimators for $\mu$: $T_1 = X_1 + X_2 - X_3$, $T_2 = 2X_1 + 3X_2 - 4X_3$, and $T_3 = (\lambda X_1 + X_2 + X_3)/3$.
                        <br>(i) Are $T_1, T_2$ unbiased? (ii) Find $\lambda$ so $T_3$ is unbiased. (iii) Is $T_3$ consistent? (iv) Which is the best estimator?</p>
                        <div class="solution">
                            <p><strong>Solution:</strong></p>
                            <p><strong>(i)</strong> $E[T_1] = \mu+\mu-\mu = \mu$. Yes, unbiased. $E[T_2] = 2\mu+3\mu-4\mu = \mu$. Yes, unbiased.</p>
                            <p><strong>(ii)</strong> $E[T_3] = \frac{1}{3}(\lambda\mu+\mu+\mu) = \mu \implies \lambda\mu+2\mu=3\mu \implies \lambda=1$.</p>
                            <p><strong>(iii)</strong> With $\lambda=1$, $T_3=\bar{X}$. The sample mean is consistent by the Law of Large Numbers. Yes.</p>
                            <p><strong>(iv)</strong> Compare variances: $Var(T_1)=3\sigma^2$, $Var(T_2)=(4+9+16)\sigma^2=29\sigma^2$, $Var(T_3)=Var(\bar{X})=\frac{3\sigma^2}{9}=\frac{\sigma^2}{3}$. $T_3$ has the minimum variance, so it is the best (most efficient) estimator.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 3: Sufficiency in a Normal Distribution</h3>
                        <p>Let $x_1, \dots, x_n$ be a random sample from a $N(\mu, \sigma^2)$ population. Find sufficient estimators for $\mu$ and $\sigma^2$.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> The joint PDF is $L = \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right]$.
                            <br>After expanding the exponent, we can write the likelihood as $L = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x})$ where the function $g$ depends on the data only through the statistics $T_1(\mathbf{X}) = \sum X_i$ and $T_2(\mathbf{X}) = \sum X_i^2$. Therefore, by the Factorization Theorem, the two-dimensional statistic $T(\mathbf{X}) = (\sum X_i, \sum X_i^2)$ is jointly sufficient for $(\mu, \sigma^2)$.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 4: Sufficiency with PDF $f(x, \theta) = \theta x^{\theta-1}$</h3>
                        <p>Let $X_1, \dots, X_n$ be a random sample from a population with p.d.f. $f(x, \theta) = \theta x^{\theta-1}$ for $0 < x < 1, \theta > 0$. Show that $t_1 = \prod X_i$ is sufficient for $\theta$.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> The joint PDF (likelihood function) is $L(\theta) = \prod_{i=1}^n f(x_i, \theta) = \prod_{i=1}^n \theta x_i^{\theta-1}$.
                            $$ L(\theta) = \theta^n \left(\prod_{i=1}^n x_i\right)^{\theta-1} = \theta^n \left(\prod x_i\right)^{\theta} \left(\prod x_i\right)^{-1} $$
                            We can factor this as:
                            $$ L(\theta) = \underbrace{\left( \theta^n \left(\prod x_i\right)^{\theta} \right)}_{g(t_1(\mathbf{x}), \theta)} \cdot \underbrace{\left( \frac{1}{\prod x_i} \right)}_{h(\mathbf{x})} $$
                            Since the function $g$ depends on the data only through $t_1(\mathbf{x}) = \prod X_i$ and $h(\mathbf{x})$ does not depend on $\theta$, by the Factorization Theorem, $t_1 = \prod X_i$ is a sufficient statistic for $\theta$.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 5: Sufficiency in a Bernoulli Distribution</h3>
                        <p>Let $X_1, \dots, X_n$ be a random sample from a Bernoulli distribution with PMF $f(x, \theta) = \theta^x(1-\theta)^{1-x}$ for $x=0,1$. Show that $\sum X_i$ is a complete sufficient statistic for $\theta$.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> From a previous example, we know $T = \sum X_i$ is a sufficient statistic. To show completeness, we must show that if $E[g(T)]=0$ for all $\theta$, then $P(g(T)=0)=1$. The statistic $T = \sum X_i$ follows a Binomial distribution, $T \sim Bin(n, \theta)$.
                            $$ E[g(T)] = \sum_{k=0}^n g(k) \binom{n}{k} \theta^k (1-\theta)^{n-k} = 0 $$
                            This equation is a polynomial in $\theta$ and $(1-\theta)$. For it to be zero for all $\theta$ in $(0,1)$, all of its coefficients must be zero. This implies that $g(k)=0$ for all $k=0,1,\dots,n$. Therefore, $T=\sum X_i$ is a complete sufficient statistic.</p>
                        </div>
                    </div>
                    
                    <div class="exam-question">
                        <h3>Problem 6: Consistency of a Squared Estimator</h3>
                        <p>If $T_n$ is a consistent estimator of $\gamma(\theta)$, then show $T_n^2$ is a consistent estimator of $\{\gamma(\theta)\}^2$.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> This is a direct application of the Invariance Property of Consistent Estimators.
                            <br>Let the consistent estimator be $\hat{\theta}_n = T_n$ and the parameter be $\theta' = \gamma(\theta)$. We are given that $T_n$ is consistent for $\gamma(\theta)$.
                            <br>Let the function be $g(x) = x^2$. This function is continuous everywhere.
                            <br>According to the invariance property, since $T_n$ is a consistent estimator for $\gamma(\theta)$ and $g(x)=x^2$ is a continuous function, then $g(T_n) = T_n^2$ is a consistent estimator for $g(\gamma(\theta)) = \{\gamma(\theta)\}^2$.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 7: Consistency of Sample Variance</h3>
                        <p>Show that the sample variance is a consistent estimator for the population variance of a normal population.</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> Let the population be $N(\mu, \sigma^2)$. The sample variance is $S^2 = \frac{1}{n-1}\sum(X_i-\bar{X})^2$.
                            <br>1. We know $E[S^2] = \sigma^2$, so the bias is 0.
                            <br>2. The variance of the sample variance for a normal population is $Var(S^2) = \frac{2\sigma^4}{n-1}$.
                            <br>As $n \to \infty$, $Var(S^2) = \frac{2\sigma^4}{n-1} \to 0$.
                            <br>Since the bias is 0 and the variance approaches 0, $S^2$ is a consistent estimator for $\sigma^2$.</p>
                        </div>
                    </div>

                    <div class="exam-question">
                        <h3>Problem 8: Best Unbiased Linear Combination</h3>
                        <p>If $T_1$ and $T_2$ are unbiased estimators of the parameter $\theta$ with variances $\sigma_1^2, \sigma_2^2$ and correlation $\rho$, what is the best unbiased linear combination of $T_1$ and $T_2$ and what is its variance?</p>
                        <div class="solution">
                            <p><strong>Solution:</strong> Let the linear combination be $T = \lambda T_1 + (1-\lambda)T_2$. This is unbiased since $E[T] = \lambda E[T_1] + (1-\lambda)E[T_2] = \lambda\theta + (1-\lambda)\theta = \theta$.
                            <br>The variance is $Var(T) = \lambda^2\sigma_1^2 + (1-\lambda)^2\sigma_2^2 + 2\lambda(1-\lambda)Cov(T_1, T_2)$.
                            <br>Since $\rho = \frac{Cov(T_1, T_2)}{\sigma_1\sigma_2}$, we have $Cov(T_1, T_2) = \rho\sigma_1\sigma_2$.
                            <br>$Var(T) = \lambda^2\sigma_1^2 + (1-2\lambda+\lambda^2)\sigma_2^2 + (2\lambda-2\lambda^2)\rho\sigma_1\sigma_2$.
                            <br>To minimize this variance, we differentiate with respect to $\lambda$ and set to 0.
                            <br>$\frac{d(Var(T))}{d\lambda} = 2\lambda\sigma_1^2 - 2\sigma_2^2 + 2\lambda\sigma_2^2 + 2\rho\sigma_1\sigma_2 - 4\lambda\rho\sigma_1\sigma_2 = 0$.
                            <br>Solving for $\lambda$ gives the optimal value: $\lambda_{opt} = \frac{\sigma_2^2 - \rho\sigma_1\sigma_2}{\sigma_1^2 + \sigma_2^2 - 2\rho\sigma_1\sigma_2}$.
                            <br>The best estimator is $T_{best} = \lambda_{opt} T_1 + (1-\lambda_{opt})T_2$. Substituting $\lambda_{opt}$ back into the variance formula gives the minimum variance.
                            </p>
                        </div>
                    </div>

                </section>
            </div>
        </main>
    </div>

    <script>
        function openTab(evt, tabName) {
            // Get all elements with class="tab-content" and hide them
            const tabcontent = document.getElementsByClassName("tab-content");
            for (let i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
            }

            // Get all elements with class="tab-button" and remove the class "active"
            const tablinks = document.getElementsByClassName("tab-button");
            for (let i = 0; i < tablinks.length; i++) {
                tablinks[i].className = tablinks[i].className.replace(" active", "");
            }

            // Show the current tab, and add an "active" class to the button that opened the tab
            document.getElementById(tabName).style.display = "block";
            evt.currentTarget.className += " active";
            
            // Re-render KaTeX for the newly displayed content
            renderMathInElement(document.getElementById(tabName), {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ],
                throwOnError : false
            });
        }
        // To ensure the first tab's math is rendered on initial load
        document.addEventListener("DOMContentLoaded", function() {
            openTab({currentTarget: document.querySelector('.tab-button.active')}, 'foundations');
        });
    </script>

</body>
</html>
